{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MODEL_PATH = \"models/autoencoder/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(28, 28, 1))                                    # 28 x 28 x 1\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(input_img)    # 28 x 28 x 8\n",
    "x_M1 = MaxPooling2D((2, 2), padding='same')(x)                          # 14 x 14 x 8\n",
    "x_C2 = Conv2D(4, (3, 3), activation='relu', padding='same')(x_M1)       # 14 x 14 x 4\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x_C2)                    # 7 x 7 x 4 = (28 x 28 x 1) * 0.25 (and each feature map has the same replicated weights so it decreases again the complexity)\n",
    "\n",
    "x_C3 = Conv2D(4, (3, 3), activation='relu', padding='same')(encoded)    # 7 x 7 x 4\n",
    "x_U1 = UpSampling2D((2, 2))(x_C3)                                       # 14 x 14 x 4\n",
    "x_C4 = Conv2D(8, (3, 3), activation='relu', padding='same')(x_U1)      # 14 x 14 x 8\n",
    "x_U2 = UpSampling2D((2, 2))(x_C4)                                       # 28 x 28 x 8\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x_U2) # 28 x 28 x 1\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(8)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(8)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_M1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(4)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_C2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(7), Dimension(7), Dimension(4)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(7), Dimension(7), Dimension(4)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_C3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(4)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_U1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(8)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_C4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(8)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_U2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(1)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = tf.keras.models.load_model(MODEL_PATH + 'autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 52s 870us/sample - loss: 0.1290 - val_loss: 0.1278\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 62s 1ms/sample - loss: 0.1289 - val_loss: 0.1277\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1288 - val_loss: 0.1277\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1288 - val_loss: 0.1276\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1287 - val_loss: 0.1275\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1287 - val_loss: 0.1275\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1286 - val_loss: 0.1274\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1285 - val_loss: 0.1274\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1285 - val_loss: 0.1273\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 74s 1ms/sample - loss: 0.1284 - val_loss: 0.1272\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 75s 1ms/sample - loss: 0.1284 - val_loss: 0.1272\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 76s 1ms/sample - loss: 0.1283 - val_loss: 0.1271\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 75s 1ms/sample - loss: 0.1282 - val_loss: 0.1270\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 72s 1ms/sample - loss: 0.1282 - val_loss: 0.1270\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 69s 1ms/sample - loss: 0.1281 - val_loss: 0.1269\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1280 - val_loss: 0.1269\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 69s 1ms/sample - loss: 0.1280 - val_loss: 0.1268\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1279 - val_loss: 0.1267\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1278 - val_loss: 0.1267\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1278 - val_loss: 0.1266\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1277 - val_loss: 0.1265\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1277 - val_loss: 0.1265\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1276 - val_loss: 0.1264\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1275 - val_loss: 0.1263\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 69s 1ms/sample - loss: 0.1275 - val_loss: 0.1263\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 68s 1ms/sample - loss: 0.1274 - val_loss: 0.1262\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1273 - val_loss: 0.1261\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 68s 1ms/sample - loss: 0.1273 - val_loss: 0.1261\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1272 - val_loss: 0.1260\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1271 - val_loss: 0.1260\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1271 - val_loss: 0.1259\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1270 - val_loss: 0.1258\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1269 - val_loss: 0.1258\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 66s 1ms/sample - loss: 0.1269 - val_loss: 0.1257\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1268 - val_loss: 0.1256\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 66s 1ms/sample - loss: 0.1267 - val_loss: 0.1256\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 66s 1ms/sample - loss: 0.1267 - val_loss: 0.1255\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1266 - val_loss: 0.1254\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1265 - val_loss: 0.1254\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 66s 1ms/sample - loss: 0.1265 - val_loss: 0.1253\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1264 - val_loss: 0.1252\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 68s 1ms/sample - loss: 0.1263 - val_loss: 0.1252\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1263 - val_loss: 0.1251\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1262 - val_loss: 0.1250\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 66s 1ms/sample - loss: 0.1262 - val_loss: 0.1250\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 62s 1ms/sample - loss: 0.1261 - val_loss: 0.1249\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1260 - val_loss: 0.1249\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1260 - val_loss: 0.1248\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1259 - val_loss: 0.1247\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1258 - val_loss: 0.1247\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 66s 1ms/sample - loss: 0.1258 - val_loss: 0.1246\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1257 - val_loss: 0.1245\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1256 - val_loss: 0.1245\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1256 - val_loss: 0.1244\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1255 - val_loss: 0.1243\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1254 - val_loss: 0.1243\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1254 - val_loss: 0.1242\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1253 - val_loss: 0.1241\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1252 - val_loss: 0.1241\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1252 - val_loss: 0.1240\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1251 - val_loss: 0.1239\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1250 - val_loss: 0.1239\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1250 - val_loss: 0.1238\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1249 - val_loss: 0.1238\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 66s 1ms/sample - loss: 0.1249 - val_loss: 0.1237\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1248 - val_loss: 0.1236\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1247 - val_loss: 0.1236\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.1247 - val_loss: 0.1235\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.1246 - val_loss: 0.1234\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 64s 1ms/sample - loss: 0.1245 - val_loss: 0.1234\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.1245 - val_loss: 0.1233\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 64s 1ms/sample - loss: 0.1244 - val_loss: 0.1232\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 64s 1ms/sample - loss: 0.1243 - val_loss: 0.1232\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 64s 1ms/sample - loss: 0.1243 - val_loss: 0.1231\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 64s 1ms/sample - loss: 0.1242 - val_loss: 0.1231\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 64s 1ms/sample - loss: 0.1242 - val_loss: 0.1230\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.1241 - val_loss: 0.1229\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 64s 1ms/sample - loss: 0.1240 - val_loss: 0.1229\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1240 - val_loss: 0.1228\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.1239 - val_loss: 0.1228\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 64s 1ms/sample - loss: 0.1238 - val_loss: 0.1227\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1238 - val_loss: 0.1226\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 60s 994us/sample - loss: 0.1237 - val_loss: 0.1226\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 64s 1ms/sample - loss: 0.1237 - val_loss: 0.1225\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.1236 - val_loss: 0.1224\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.1235 - val_loss: 0.1224\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.1235 - val_loss: 0.1223\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.1234 - val_loss: 0.1223\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.1233 - val_loss: 0.1222\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.1233 - val_loss: 0.1221\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.1232 - val_loss: 0.1221\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 62s 1ms/sample - loss: 0.1232 - val_loss: 0.1220\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.1231 - val_loss: 0.1220\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 62s 1ms/sample - loss: 0.1230 - val_loss: 0.1219\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.1230 - val_loss: 0.1218\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.1229 - val_loss: 0.1218\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.1229 - val_loss: 0.1217\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 62s 1ms/sample - loss: 0.1228 - val_loss: 0.1217\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.1227 - val_loss: 0.1216\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.1227 - val_loss: 0.1215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29a139e93c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=100,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "autoencoder.save(MODEL_PATH + 'autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/gAAADjCAYAAAAxIr9SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xm8lfP6//Frm5KQSiKakUrKmCFDeBgzHSFDx3BM5+DEMf944BiO70PkOKZwDpKjU2SWZGiQuSKaJGmei0hFtH9/eHR5fz7ttdrt1tp7rc9+Pf+6bp97r3237v25131b1/W5SkpLSw0AAAAAABS3Dar6AAAAAAAAwPrjAR8AAAAAgATwgA8AAAAAQAJ4wAcAAAAAIAE84AMAAAAAkAAe8AEAAAAASMBG2QZLSkrooVd1FpaWltbPxQtxHqtOaWlpSS5eh3NYpZiLCWAuJoG5mADmYhKYiwlgLiahzLnIN/iFa1pVHwAAM2MuAoWCuQgUBuYiUBjKnIs84AMAAAAAkAAe8AEAAAAASAAP+AAAAAAAJIAHfAAAAAAAEsADPgAAAAAACeABHwAAAACABPCADwAAAABAAjaq6gMoj6uuusrjmjVrBmO77babx126dMn4Gg8//LDHH3zwQTDWp0+f9T1EAAAAAACqFN/gAwAAAACQAB7wAQAAAABIAA/4AAAAAAAkoGBr8Pv16+dxttp6tWrVqoxjF110kceHH354MDZs2DCPp0+fXt5DRBXbeeedg+2JEyd63L17d4/vv//+Sjum6qxWrVoe9+jRw2Ode2Zmo0aN8viUU04JxqZNm5anowMAAKgaderU8bhx48bl+pn4nuiKK67weOzYsR5PmjQp2G/MmDEVOUQkhG/wAQAAAABIAA/4AAAAAAAkoGBS9DUl36z8afmalv3GG2943Lx582C/4447zuMWLVoEY2eeeabHd955Z7l+L6re7rvvHmxricbMmTMr+3Cqve22287jCy64wOO4dGbPPff0uHPnzsHYgw8+mKejg9pjjz08fv7554Oxpk2b5u33HnHEEcH2hAkTPJ4xY0befi/WTj8jzcxefvlljy+99FKPe/XqFez366+/5vfAErTNNtt43L9/f4/ff//9YL9HH33U46lTp+b9uFarXbt2sH3QQQd5PGjQII9XrlxZaccEFINjjz3W4+OPPz4YO+SQQzzecccdy/V6cep9kyZNPK5Ro0bGn9twww3L9fpIF9/gAwAAAACQAB7wAQAAAABIQJWm6O+1114en3TSSRn3GzdunMdxysvChQs9Xrp0qcebbLJJsN+HH37ocbt27YKxevXqlfOIUUjat28fbP/4448ev/DCC5V9ONVO/fr1g+3evXtX0ZFgXR155JEeZ0vzy7U4Dfy8887zuGvXrpV2HPiNfvY99NBDGfd74IEHPH788ceDseXLl+f+wBKjq2ebhfc0mg4/b968YL+qSsvXTidm4bVeS6wmT56c/wMrMltuuWWwrWWfu+66q8dxNyfKHQqblvZecsklHms5oplZzZo1PS4pKVnv3xt3iwLKi2/wAQAAAABIAA/4AAAAAAAkgAd8AAAAAAASUKU1+NpWK65V0Ro1rRedM2dOuV77yiuvDLZbt26dcd/XXnutXK+Jqqc1bNq6ycysT58+lX041c5f//pXj0888cRgbJ999lnn19P2S2ZmG2zw+/9zHDNmjMfDhw9f59dGaKONfr/cH3PMMVVyDHFt79/+9jePa9WqFYzpmhrID51/O+ywQ8b9+vbt6/GKFSvyekyp2HrrrT2O2wDXrVvXY1374LLLLsv/gWVw4403etysWbNg7KKLLvKYuvs1aavlO+64Ixhr1KhRmT8T1+ovWrQo9weGnNHrY/fu3fP6u7T9tz4LIXe0TaFeq83CNeG0taFZ2PZZW8a+9957wX6FcJ3kG3wAAAAAABLAAz4AAAAAAAmo0hT9V155xWNNlzAz++GHHzxevHjxOr923HJp4403XufXQOHZZZddPI5TeuM0SOTevffe67GmKlXUH/7wh4zb06ZN8/i0004L9otTvbF2nTp18ni//fbz+K677qq0Y4jbhWnp1GabbRaMkaKfe3FLxBtuuKFcP6flT6WlpTk9plTtscceHsdpnurWW2+thKNZU5s2bYJtLWuM28zy2bomTdn+5z//6XHcdjnTfLn//vuDbS05rMg9L8onTsfWdHtNsx40aFCw308//eTxkiVLPI4/p/S+dPDgwcHY2LFjPf7oo488/vTTT4P9tPUon4MVpyW9ZuEc03vN+G+ivDp06ODxL7/8Eox9+eWXHo8YMSIY07+5n3/+uUK/uzz4Bh8AAAAAgATwgA8AAAAAQAJ4wAcAAAAAIAFVWoOvtN62oq6++mqPd95554z7ae1LWdsoXNdcc43H8d/MyJEjK/twqoWBAwd6rG3sKkrbAS1dujQYa9Kkicfaqunjjz8O9ttwww3X+zhSF9efaauzr7/+2uN//OMflXZMJ5xwQqX9Lqypbdu2wfaee+6ZcV+tKXz99dfzdkyp2GabbYLtk08+OeO+f/rTnzxesGBB3o4ppnX3b731Vsb94hp8XRMJv7nqqqs81raH5RWvK3PUUUd5HLfa03r9fNbspipbXXy7du081vZosQ8//NBjXV9j6tSpwX6NGzf2eObMmcFYLtYtwpp22203jy+55BKP4zkWt6ZcbdasWcH2u+++6/E333wTjOkziK4FFbeJ1mtC3JZYW0Brq71c4xt8AAAAAAASwAM+AAAAAAAJKJgU/Yrq3Lmzx9puZpNNNgn2mz9/vsfXX399MLZs2bI8HR3WV9OmTYPtvfbay+NJkyYFY7QTyY2DDz442G7ZsqXHmmJW3nSzOAVJU+S03YyZ2aGHHupxthZef/7znz1++OGHy3Uc1c2NN94YbGuaoqaDxmUSuaapavHfFimLlStb2ngsTmVFdvfcc0+wfdZZZ3kct/V89tlnK+WYYgceeKDHDRo0CMaefPJJj59++unKOqSioeVjZmbnnntumft9/vnnwfa8efM8PvzwwzO+fu3atT3W9H8zs//+978ez507d+0HW83F9//PPPOMx5qSbxaWqGUrW1FxWr6aPn16uV4DFffII48E21paka3l3dtvv+3xF1984fH/+3//L9hvxYoVGV9j//3391jvQx9//PFgv/bt23us1wAzswcffNDjAQMGeJzrci2+wQcAAAAAIAE84AMAAAAAkICiT9HXlO04LUf169fP42HDhuX1mJA7cUqvqszVh1OnpRD/+9//grFsKU9Kuxpo2tHf//73YL9sJTH6GhdeeKHH9evXD/a76667PN50002DsQceeMDjlStXru2wk9KlSxeP45VbJ0+e7HFldpzQUos4JX/o0KEef/fdd5V1SNXWQQcdlHEsXp07W4kM1lRaWhps69/67Nmzg7F8roRes2bNYFvTT//yl794HB/veeedl7djSoGm3JqZbbHFFh7rqtvxPYt+Pp1++ukex2nBLVq08HjbbbcNxl566SWPjz76aI8XL15crmOvDjbffHOP4zJcLeVduHBhMHb33Xd7TLlu4Yjv63T1+vPPPz8YKykp8VifC+LyzR49enhc0ZLeevXqeazdnG655ZZgv0GDBnkcl/dUFr7BBwAAAAAgATzgAwAAAACQAB7wAQAAAABIQNHV4L/44ovB9hFHHFHmfk899VSwHbeMQnFo27ZtxjGtw8b62Wij3y8F5a25j9ey6Nq1q8dxnVt5aQ3+nXfe6XHPnj2D/TbbbDOP47+Dl19+2eOvv/66QsdRrE455RSP9T0yM3vooYcq7Th0TYczzzzT419//TXY7/bbb/e4uq2XUFm0rY/Gsbgm8bPPPsvbMVU3xx57bLCtLQh17YmKtvzUuu9DDjkkGNt3333L/JnnnnuuQr+ruqpRo0awrWsY3HvvvRl/TltuPfHEEx7rtdrMrHnz5hlfQ2vD87l+QzE78cQTPb7uuuuCMW1dp60izdZs1YvCEF/Hrr76ao+15t7MbNasWR5rK9iPP/64Qr9ba+sbNWoUjOmz5cCBAz2uU6dOxteLj7dPnz4e53PtIb7BBwAAAAAgATzgAwAAAACQgKJI0d9uu+08jlMMNW1K04I19dPMbOnSpXk6OuSaphSee+65wdinn37q8Ztvvllpx4TfaHu1uK1SRdPyM9FUe03zNjPbe++9c/q7ilXt2rWD7UzpuGYVT/+tCG1xqCUfEyZMCPYbMmRIpR1TdVXeuVKZfx8puu+++4LtTp06edywYcNgTNsVavrm8ccfX6Hfra8Rt79TU6ZM8Thu04bstMVdTEsw4jLSTLTF89p8+OGHHnMvW7Zs5Ud63zhz5szKOBysJ02TN1uzvE/98ssvHnfo0MFjbRtsZrbLLruU+fPLly8Ptlu1alVmbBbe5zZo0CDjMal58+YF25VVmsg3+AAAAAAAJIAHfAAAAAAAElAUKfoDBgzwuF69ehn3e/rppz2ubqtnp+Twww/3uG7dusHYoEGDPNbVaZE7G2yQ+f/7afpTvmnaaXxM2Y7xlltu8bhbt245P65CEq/svP3223vct2/fyj4c16JFizL/+9ixYyv5SJAtFTgXK7jjN6NGjQq2d9ttN4/bt28fjB111FEe6+rQCxYsCPbr3bt3uX63rso8ZsyYjPu9//77HnOPtG7i66mWU2gZTJwGrJ2ATjrpJI/jVbd1LsZjF1xwgcd6rsePH1+uY68O4nRspfPt5ptvDsZeeuklj+kcUjjeeeedYFvL+fQZwcyscePGHv/rX//yOFu5kqb8x+UA2WRKy1+1alWw/cILL3j817/+NRibM2dOuX/f+uAbfAAAAAAAEsADPgAAAAAACeABHwAAAACABJRkq1EoKSnJPJhnWt/Uv39/jzfeeONgv6FDh3p8wgkneJxAK5FRpaWl5e+jkkVVnseKePbZZz0++eSTgzHd1hqXQlVaWlqy9r3WLt/n8O677/a4e/fuGfeL518+XXbZZR737NkzGNMa/Lj2SWsgc1RnWrBzsWbNmsH2u+++63F8rrRt1+LFi3N5GLbNNtsE25lqzOJatAcffDCnx5FNsczFXOjYsaPHw4YN8zheu2LatGkeN23aNO/HlQMFOxerUvPmzT2ePHlyMKZ1xUceeaTHcb1/ZSrGuRivB6Tvs7Yr1bVjzDLXAb/11lvB9iWXXOLxq6++GozttNNOHj/22GMeX3zxxWs77HwqqLmo73N8T5CN7turVy+PtTWhWVjnred+3LhxGV+7TZs2wfYHH3zgcaG06yvGubjVVlsF29ddd53HBxxwgMeLFi0K9ps+fbrHun5Ru3btgv322WefdT4m/dsxC9uQ6voaeVLmXOQbfAAAAAAAEsADPgAAAAAACSiYNnlx+ztNb8iWFqzpZwmk5Vdb2267rccHHnigx19++WWwXzGk5Rej4447rkp+b/369YPt1q1be6zXgGziVNOVK1eu/4EVieXLlwfbWpIQl7e89tprHsclD+Wx6667BtuaFhynd2dKS12X1ElUnH6eZmsp+eabb1bG4SDPbrrpJo/juXfttdd6XJVp+cUuLms69dRTPX7uuec81nT92P333++xnhezsO3v888/H4xpCrKWWcTtSKtz60MtM/zb3/5W7p/T6+Nf/vKXMuNc0fmn5cVdu3bN+e9KWZzyrvOjIp566qlgO1uK/g8//OCx/p09+eSTwX7ahq+q8A0+AAAAAAAJ4AEfAAAAAIAE8IAPAAAAAEACCqYG/8orrwy299577zL3e/HFF4Ptm2++OW/HhMpzzjnneKwtt15//fUqOBpUlhtuuCHY1lZB2UydOtXjs88+OxjTVijVjV4P43ZNxx57rMd9+/Zd59deuHBhsK21vltvvXW5XiOuU0N+dOnSpcz/HtcuPvLII5VxOMixU045Jdj+4x//6LHWiJqt2SoKuaFt7nS+nXHGGcF+Oud0rQStuY/ddtttwXarVq081hbS+npma34WVidah92vX79g7JlnnvF4o43Cx55GjRp5nG29klzQNYf0b+bGG28M9rv99tvzehwwu+aaazxelzUQtDVlRe6jKhPf4AMAAAAAkAAe8AEAAAAASEDBpOiXt63FpZdeGmzTGi8NTZo0KfO/f/vtt5V8JMi3gQMHetyyZcsKvcb48eM9HjFixHofUyomTpzosbZxMjNr3769xzvuuOM6v7a2gor17t072D7zzDPL3C9u64fc2GGHHYLtOE14tZkzZwbbI0eOzNsxIX+OPvrojGOvvvpqsD169Oh8H061p+n6GldUfJ3UlHNN0e/UqVOwX926dT2O2/qlTtuSxde1nXfeOePPHXbYYR5rS+5bbrkl2C9T2XBFaQndnnvumdPXRtnOP/98j7UsIi7bUOPGjQu24xaWhYxv8AEAAAAASAAP+AAAAAAAJKBgUvTLS1OQzMxWrly5zq+xZMmSjK+hKTq1a9fO+BpbbbVVsF3eEgNNI7r22muDsWXLlpXrNVLUuXPnMv/7K6+8UslHUj1puli2lWSzpYY++uijHjds2DDjfvr6q1atKu8hBo477rgK/Vx19tlnn5UZ58KUKVPKtd+uu+4abI8dOzanx1Fd7b///sF2pjkcd6FBcYqvwz/++KPH99xzT2UfDvKsf//+HmuK/mmnnRbspyWst956a/4PLAFvv/12mf9dS9rMwhT9X375xeMnnngi2O+xxx7z+PLLLw/GMpVOIT/22WefYFuvjZtvvnnGn9PSb10138zsp59+ytHR5R/f4AMAAAAAkAAe8AEAAAAASAAP+AAAAAAAJKDoavA///zz9X6NZ599NtieM2eOxw0aNPA4rm/Ktblz5wbbd9xxR15/XyHp2LFjsL3ttttW0ZHAzOzhhx/2+K677sq4n7ZgylY/X97a+vLu16tXr3Lth6qhaziUtb0aNff5Ua9evYxjCxcu9Pi+++6rjMNBHmgtqN6nmJnNnz/fY9ripUc/J/Xz+YQTTgj2u/nmmz3+3//+F4xNmjQpT0eXpsGDBwfben+ubdUuuOCCYD9tQXvIIYeU63fF7UuRG/FaTVtssUWZ++kaJmbhOhfvvfde7g+skvANPgAAAAAACeABHwAAAACABBRMiv7AgQOD7Tj1KJdOOeWUCv2ctsbIllr88ssvezxy5MiM+7377rsVOo4UnHTSScH2hhtu6PGnn37q8fDhwyvtmKqz559/3uOrr746GKtfv37efu+CBQuC7QkTJnh84YUXeqxlNCg8paWlWbeRX0ceeWTGsenTp3sct4hF8dAU/Xh+vfbaaxl/TtNS69Sp47H+XaB4aIvTm266KRjr0aOHx//4xz+CsW7dunm8fPnyPB1dOvRexCxsVXjqqadm/LlOnTplHNM22Tpnr7vuuoocIsqg17trrrmmXD/z3//+N9geOnRoLg+pyvANPgAAAAAACeABHwAAAACABPCADwAAAABAAgqmBv8Pf/hDsK21ExtvvHG5XqNNmzYer0uLu8cff9zjqVOnZtxvwIABHk+cOLHcr4/fbLbZZh4fc8wxGfd77rnnPNaaJeTPtGnTPO7atWswduKJJ3rcvXv3nP7euDXkgw8+mNPXR+XYdNNNM45R75kf+rnYokWLjPutWLHC45UrV+b1mFA19HPyzDPPDMauuOIKj8eNG+fx2Wefnf8DQ1499dRTwfZFF13kcXxPfeutt3qci3bTqYs/ty6//HKPN998c4/32muvYL9tttnG4/h5ok+fPh7fcsstOThKmIXnY/z48R5ne3bUOaDnNiV8gw8AAAAAQAJ4wAcAAAAAIAEl2doZlZSU0Ouo6owqLS3da+27rV2hnEdNlxk2bFgwNn/+fI/POOMMj5ctW5b/A8uj0tLSkly8TqGcw6OOOspjbWNnZnbcccd5rK0iH3300WC/kpLf3xJNpzIr2NZNyc3FXJs7d26wvdFGv1d/3XbbbR7fd999lXZMsdTmorYW/fe//x2MnXPOOR5rGm8CadnVdi5qe7S2bdsGY3pNje/p/vOf/3isc3HGjBm5PsRyS20uForGjRt7HKeH9+3b1+O4jKOCqu1cVNp+0Mxs33339fjvf/97MKb3uYUihbl4/PHHe/zSSy95nO359rDDDvN4yJAh+TmwylPmXOQbfAAAAAAAEsADPgAAAAAACSBFv3CR/pSAFNKfwFxcm1deeSXY7tmzp8eFkv6W8lxs2LBhsH377bd7PGrUKI8T6FJRbedix44dPdYV0c3Mhg8f7vHDDz8cjH377bce//zzz3k6unWT8lwsFIMHDw6299tvP487dOjgcVwmtw6q7VxMSQpzccyYMR7H5UuqR48eHl977bV5PaZKRoo+AAAAAACp4gEfAAAAAIAE8IAPAAAAAEACqMEvXNQ3JSCF+iYwF1PAXEwCczEBzMX823LLLYNtrVPu3r27x9rSdh0xFxOQwlzUlp877LCDx3Fbwvbt23s8Z86c/B9Y5aEGHwAAAACAVPGADwAAAABAAjaq6gMAAAAAkBvff/99sN2sWbMqOhIgv7Qtr8a33XZbsF9iaflrxTf4AAAAAAAkgAd8AAAAAAASwAM+AAAAAAAJoE1e4aIFSQJSaEEC5mIKmItJYC4mgLmYBOZiApiLSaBNHgAAAAAAqeIBHwAAAACABKytTd5CM5tWGQeCNTTJ4WtxHqsG5zANnMfixzlMA+ex+HEO08B5LH6cwzSUeR6z1uADAAAAAIDiQIo+AAAAAAAJ4AEfAAAAAIAE8IAPAAAAAEACeMAHAAAAACABPOADAAAAAJAAHvABAAAAAEgAD/gAAAAAACSAB3wAAAAAABLAAz4AAAAAAAngAR8AAAAAgATwgA8AAAAAQAJ4wAcAAAAAIAE84AMAAAAAkAAe8AEAAAAASAAP+AAAAAAAJIAHfAAAAAAAEsADPgAAAAAACeABHwAAAACABPCADwAAAABAAnjABwAAAAAgATzgAwAAAACQAB7wAQAAAABIwEbZBktKSkpLSkrMzKy0tLRSDghuYWlpaf1cvFBJSUnpBhv89v9yVq1alYuXRDmVlpaW5OJ1OIdVirmYAOZiEnI6F7m/qRrMxSQwFxPAXExCmXNxbQ/4tummm5qZ2YoVK4KxfE/C1ZO9vP99XWQ79gK6uEzL1QttsMEGVrNmTTMzW758eTDGRCwOG2ywgc/F+BwW0N9sqnI6FzmPxY3raZXK2Vysyvsb5AbX0yrFXCxS+XgQZy5WqTLnIin6AAAAAAAkIOs3+KWlpWv8n5jKov/HR7+1j/9PUHm/0S+Sb+3zYtWqVbZs2TIzS//fmqpVq1b5XOQcFi/OY/HjepqGqry/QW5wPU0Dc7Fy5SPTjLlYePgGHwAAAACABPCADwAAAABAAnjABwAAAAAgAVlr8CtTXEtfo0YNj2vVquXx6lUay/q51StDmq1ZA/Lzzz97vLp+cjWt/fnll1/W5bCLBjUxxY9zmAbOY/HjHAKFgbkIrJt8tSRkLhYWvsEHAAAAACABPOADAAAAAJCAKk3R15T65s2bB2PHHXecx+3bt/d4yy23DPbTFP0NN9ww4+9asGCBx1988UUw9vbbb3s8fvx4j1NN109FfL5r1qzpsaYKxe1X8tEiBGYbbfT75UTn6TbbbBPsp/Nq9uzZwZieK9K9gKqjn8+K6ycArF1ceqzX1I033tjjuPRYt7VcOX7NpUuXlhmbhfdZmdqL84yTNr7BBwAAAAAgATzgAwAAAACQAB7wAQAAAABIQJXW4G+xxRYen3LKKcFYt27dPK5Xr57HcU2L0prseL8ff/zR45122inja0yZMsXjuKYFVU9rmBo0aBCMHXPMMR5rfdOzzz4b7Ldw4cI8HV31ojX3ZmZNmzb1+Nxzz/X4oIMOCvb77rvvPH7ggQeCsaFDh3r8008/5eAoUZZs11HWPqg+9O9g8803D8Z0XRy9no4bNy7YL17jBGu3ySabeLzZZpt5HL+XXAOBwhavVaJrDm299dbBmN4j7b777h63atUq2K9u3boex+uO6X3XV1995fELL7wQ7PfJJ594rPdcZma//vqrma3ZMhxp4Rt8AAAAAAASwAM+AAAAAAAJqNQU/TiVpUWLFh4fdthhwZimX2vK6MqVK4P9tIWExnH6sKYYNmzYMBhr3bq1x5ouR4p+4dGyjhNOOCEYu+iiizz+4YcfPB4yZEiwHyn6FadzWNPNzMwuvvhij0899VSP4zZ5mnaqpTNmZpMnT/ZYy2VIG19/2nqnSZMmHsfX1KlTp3qci5Zo5W1lGv8u2rHlXlyaoSmkXbp0CcbOOussj/VcXHHFFcF+I0eOzOUhJklbuJqFZUs77rijx6+++mqw37Rp0/J7YEKv7XofZBbeT+ln6+pUX/wunmOZyqG4vhUvnR9HH310MHbVVVd5vNVWWwVjderU8VjvZeNWeDoXs5XT7brrrh63bNkyGOvdu7fH77zzTjD2/fffmxklQKnjG3wAAAAAABLAAz4AAAAAAAmo1BT9ONVE0+bj1WNnz57t8eLFiz2OV4PUNOFtt93W43hFYE2HitOC9fVJOSsscUpv27ZtPT7xxBODMS3r0DTj+fPn5+fgqgmdt9rRQlN4zcyOPfZYj+vXr++xrhgdi1eP7dy5s8ePP/64x5oWivKJy5T0/Jx99tke62q7ZmY9evTweMWKFev9u7VEY+eddw72++WXXzyePn16MDZr1iyPuS7nhpZpmJkdf/zxHl9wwQXBmKZ8armazm1kpp9de+65ZzB2zjnneKz3NPFK2PkUl0xuv/32Hh911FHBmN6fvfzyyx6vTvWtjvRzUVO2tfTULFwFXcuhvvnmm2C/RYsWecz1rvDoedQuQd27dw/2a9SoUcbX0L+ZTHFFxffKeq3XcgCz3z9342tAdaL/9vheKVspYSZxyY3O4Xg+V1bJafU9uwAAAAAAJIQHfAAAAAAAEsADPgAAAAAACajUGvy4RkFbYj300EPBmNaMzJs3z+O4rYO2m+nUqZPH2nrGLKy3+PLLL4Ox0aNHe7xs2bLM/wBUungthUMPPdTj7bbbLhhbsmSJx88991yZ/x3rrlatWh537NjR4713FsPqAAAa8UlEQVT33jvYT2u+tNZQ66zNwnrOeEzbvnTo0MHj9957L+NroGxxe8Ju3bp5rNfHuIVOfE7KI64h1L8ZvUbHrdh0bYW33347GBswYIDHnO+K03MT18/r9VTX1zALP6/1Mzj+/MRv4jmg7/Xpp58ejGVqjfftt9/m6eh+o/dBjRs3Dsb0+qBz1sxsxIgRHtPe7Tf6eafvl9Znm2Wuwdf7TrNwzZkZM2YEY7znlUPncLxeyQEHHOCxtmiOW0rqmhrxGhU6v3VtKL2+moVrnsR18lrPrW00P/roo2A/XePh559/DsZWt+VL/e8qrq3XtoV6/YuvhdlaGOrfhY7F9016fr/44otgbNKkSR7H5yaX+AYfAAAAAIAE8IAPAAAAAEACKjVFP24NoKksQ4YMyfhzmkYSty/Q19Q2eVtvvXWwX+3atT2OWz/pcdCepOrpOdaUcDOzM844w+P4HGtrRU1xSz0NKdfiObbLLrt4fOqpp3rcpk2bYL+4FUsm2h4zTiPXVEc9v/ExaTp3RVLKU6Uphs2bNw/GmjVr5rG2LtRSKbOKXQOzpSdryy1NczQLUxFjgwcP9pgU/YrTubPffvsFY7qtn5Fm4Wernqds56w6i1N1tS1l3CZPU0XLe92sqEwlGldeeWWw38EHH5zxmDSltLJaPBWa+BqnbQW7du3qsZa9mIVpwvoa7du3D/arW7eux7fddlswtmDBAo+r6/u/PrK1pNPro6Zcx/eX2uJVW23r3DAzGz9+vMdvvPFGMDZhwgSP9ZzGn296TxOfb93OFMfb8b959b8zlXunTC0r41Lt/fff32O9JutcNgtLDGvWrBmMaYq+3kfF77GWe2sphZnZPffc43E+y5/4Bh8AAAAAgATwgA8AAAAAQAJ4wAcAAAAAIAGVWoMf01rPbHWfWtsQ15lofVPTpk093mGHHYL9tLZG69/MzBYtWuRxKjUpxUzbNV188cXBWIsWLTyOWxpqnYu2cqJmbd3EtbgXXnihx7omQjyPtN3HwoULPZ47d26wn9avaa1T/JraMu+CCy4I9ps1a5bHEydODMaq8xzWmrC99torGNO1D0aNGuXx+++/H+xXkfkSt/Jp1aqVx9pOMW7Fpscb1zzGr4mK0XrquD64Tp06HmdbI2fYsGFl/vfqTv9G4zr7888/3+MmTZoEY3qN0nuYuK1TRcS1oDrnrr/+eo91PZvYzJkzg21tO1tdWwnr9dMsXE9E16mJ22rp34i+RtwC+Pjjj/d43Lhxwdgzzzzjcdx6DWvSzxWzcK2f+Dqnc1HbGMZrjQwaNMhjXQNI26GZhddHfb2yfndliX/v6nbjxXRvrPNIW0+ahddXXVcmXvNHa+31Pjde40m349+lNfk61+P7Fa2nj9vw6d/nWWed5bE+i+YCd1AAAAAAACSAB3wAAAAAABJQpSn6FRGn9O62224e77HHHh5rOxizMH14zpw5wdiUKVM8pqVa1dC0wrZt23rcrl27YD9NYYxbi3z88cceL168ONeHmDR9/zWl2ixsc9agQQOP47KaefPmefz66697/Pzzzwf7ffvttx7HrfYOO+ywMo9jp512Cvbr3Lmzx3FqlKbs67xPUbbWTfvuu28wpu/7k08+6XEuUq7j49BSGk29j8+VtiyN01JJRc2N7bbbzmNt9WQWno/VaZur6ediv379PI7TTqszTdHU66RZeM2KW+hpmnvDhg091pIJszBNuLzptHGJ1RVXXOFxt27dPI5TT7XN7EMPPRSMvfvuu+t8HKnRNnZm4fVVU27jew8d07T8uP2Wno/42j1y5EiPP/30U49p6/w7bV8Wl8voc0Jc0qdt7X744QeP4/dWz2u29nTFoBiec+IyC30u0HbNZmYdOnTwWMsx4tR7Pad6PxQ/E+pnYaNGjYIxbceuczYuzVHxmH42aEyKPgAAAAAAWAMP+AAAAAAAJKAoUvQ1zeLggw8Oxrp27eqxpk7EqRm6qreuSGrGqsCFQFNFNY00Tn/StJpPPvkkGBs4cKDHpJGuG00h0jR5szANSc9HnNbUu3dvj/v06eOxpn6ahelh06dPD8Z0X/1dcamGpmvFc11Tuz777DOPU1xdX9MSzcLro6brm4XdDD7//HOPc5GuF58DXblbyySWLFkS7Dd69GiPX3zxxWBM0/exbvR6qisMx6u062efXlvNzIYPH+7xpEmTPC7GlNR80WtNnBqv71P8eaSponrf0rp162A/nbP6GnFJTLZSgZNPPtljTQuPu5v06tXL46effjoYS/HaWR76PutK+WZhKrB+jmkKvVl4fjXdNy4j1fMbr7CvHaImTJjgsXakqY70c2efffbxOO4Qoe9tnKKv5TL6d851rvLpfNtxxx2DsRtuuMHj+H5QO8Xo/cyCBQuC/fT+UudRPGe1PDAuI23ZsqXHOi/jDkH6txmXaGkqfj6fVfgGHwAAAACABPCADwAAAABAAnjABwAAAAAgAUVRg6+t8eL2CFpfqLUXca3ngw8+6LG2fIl/DlVDa2+0liVulaE1o6+++mow9uWXX3pM/VR2cQ2ntmrSljJmYS2R1qgNGTIk2O+BBx7wWM9TtnMRz1Otj9MaKa2vMzNr1apVxuPdddddPb7qqqs8njVrVsbjKCZ67uJ/+3nnneexnlOzsE401/Xtcfs7nbc6Fp/vQYMGeaztisy4Lq8P/RvRut+tttoq2E9rweP5MWLECI9ZD6Fs+re98cYbB2N6DuJroG5rXXzc3kuvgdqmK17zYocddvD42GOPDcYytY2K17B55JFHPK7utd2r6ZoV2u7TLLyWaTx58uRgP11TSGtx45bPup6KtrY0C9dW0fVTvvrqq2C/6nbfo++Zts495JBDgv205vmdd94JxvicKRw63zp27BiMadvkuMVnpmtt/Pygn2MzZszweOrUqcF+2i4xrp/XVqa6jkb82ar/lnhe6uvH90S5xDf4AAAAAAAkgAd8AAAAAAASULAp+pr6pum5cQqbpj5oOts///nPYL++fft6rGmJKAya3qhtKOI2YJqm+NFHHwVj2u4E2cWpS5oOpSlIZmE6kaYE9ujRI9gvbrNVHnHqkpYA6LnXNlBmYTqUtkiJtzWVP5UUfX0v/vjHPwZje+yxh8dxaytNa4vfz/UVt3Vq1qyZx5oWPH/+/GC/t956y+OlS5fm9JiqM0371vTeOPVX51/cLlZLnkhjLZu2gIxT3rVUSM+HWfi3rq2c4nRQvS5rW7v4c1Hvi7SFqFk417X904ABA4L9KnL9Tp3eh8afVXrPote7OH1Yx7RlXnwO9TNZSy7Mwvmn1/gpU6YE+1W3doZ6H7Dffvt5HJenaQvLAw44IBjTFmmats1zQuXTe4W4hEXn308//RSM6bnSa3J8TdPrrv5MfN3VlP/4flhL3rREvG7dusF+ehzapt0svPbGraJziW/wAQAAAABIAA/4AAAAAAAkoGBT9DVF6U9/+pPHcVqTpnz27NnT4+eeey7Yr7qlLhUbTaFq06aNx3Eqsaa6xCnXpJGWX5xSre95nL7/9ddfe/yvf/2rzP9eUfFq/ppen2n1Z7PsqZMrV670OMXVv/UaqOfNLEw1i1MMd9ppJ4/btWvncZw+pqll2eg56dChQzCmx6XnJy6r0fQ05m/uNG7c2GNNSY3TDTVlcdq0acGYpo6jbFoW1q9fv2Dsgw8+8Fg/38zC655+xsXXZU3pbt++vceaGmoWzu14NWe9BmqXEu2SYFb9VmAvD71vnDNnTjCmKcRaFhGn6Ov1Wq9x8fVZx+L0ZP072H333T2OOwlVtzIn/QzSOZat/OHII48MxvQ89O/f3+N4ZXVNC9fPyGznMcYcy07v3QYPHhyM6TVPr3dm4d+BvkY8Z7XcVOdp3I1I/5Z22WWXYEz31VX043Or91VPPfVUMNa7d2+Py3u/VRF8gw8AAAAAQAJ4wAcAAAAAIAE84AMAAAAAkICCqcFv0KBBsH3rrbd6vPfee3sctz0YNmyYx2+//bbH1NwXl6ZNm3qsdS1xfdNnn33m8ZIlS/J+XCnRus+4JlTrkbQVoZnZ559/7vGHH37ocS7mWFwrp7XhO++8s8dacx//7vjvQNtVac1pKvTfrq3MzML3L1st4BFHHOFxXHs9efLkMn9XvA6CrpFw0kknBWNbb721x8uXL/d46NChwX5xuxtUTDw/DjzwQI/r1auXcT+d63HNY4rrV+Sa1l1qCzozs7Fjx3ocrzWi2xtt9PttWNzyU2u7teWntmgzC9fe0PlmFtadvvTSSx5r2z2UTe8/4s8SbRmr61zotc8svA7rmg1xW1+9FsZtFfVzUv8O4t9V3WrwtbXnpEmTPNZWZmbh+xc/a3Tu3NljbUEYP2ssWrTIY507cevXH3/80eO4vlrr+seNG+exztHqLNt8u+mmmzyO/+51bRldgyReD0OfM7RlrLY0jfeL1zTRz1O9js+ePTvYT+vsn3jiiWCsslp68w0+AAAAAAAJ4AEfAAAAAIAEVGmKvqZ8HnPMMcFYp06dPNaU7bil0/vvv++xpusUI02fTL1lVJwqqi2AND1N053MzAYNGuSxtsPA2ul7rqncZmHbkTjNb/z48RnHKkLTmuK2UNqSRNOwNI3VLHtq7JgxYzxOMfVNUzl1PpiZtW7d2uO43WGmtLCWLVsG23qONd1Xr8NmZh07dvRY569ZeL70HHzxxRfBfqlf5ypLnNp90EEHeazp3PE1c+DAgR5rWzczzk0uxS2UdFvTeOPyKE3z17KqfffdN9hPU1Tja6Wmjmr5Ep+f6yb+LNF7z0MPPdTjuHxCU7u1Laj+d7Pwvkev42ZhqrFer3Wem5nNmDHD47i8MUV6f/jiiy96HH+maYu1+HNRWxDWrVs34+/ScjWds3Eafrbrpn6evvPOOx7/3//9X7BffB2ojuK/X73Pi+eilj7o/Ivbwmqphqbla1t2s7D8MJ7Per71mfOee+4J9uvTp4/HVVWKyDf4AAAAAAAkgAd8AAAAAAASwAM+AAAAAAAJqNIafK3p7NKlSzCWqQYirp+Ja7QLTdweR8Vtp7QGK8XaYVWrVq1ge8899/RYz7fWlJmFrUXiukZkp7WZLVq0CMZ0Lsb1QjVq1PBYawHjmu5MbfPiOaCvp22gzMwOO+wwjxs3blzmz5iFdeJxq7i+fft6nGIbNn2f33rrrWBMazzj9Q30/GudYPweaR2irtvQrFmzYD89d3FbJz3GefPmeVzs66QUEj03unaFWdjmR891XPfbr18/j1OcK8Um/kzTFqCzZs3yOL7W6jU2vifS66Oup8IaC+smXrNgyJAhHmtLyR133DHYT1v76jmMa4y1bdeFF14YjGmtva5Nc9pppwX7DR8+3GP9LDBL83zre/jCCy94rO+zmVnXrl09jtc30LZn+tkX36NmWiMrXk9Kxfc+es+vbYDj/a6//nqPafm9pvKuaRLfo+paNVqD37x582A/rdWPX0Pr/e+9916Pn3/++WC/eG2GqsA3+AAAAAAAJIAHfAAAAAAAElCpKfpxGoqm4Mbpn5qmrSkwmp5kFrZq0vQzbQ1jVrH0pGzpNXH6jh6XpoHE6TWayhWn9mj5gbbHSYW+n3ruzcx22203jzWVeMGCBcF+pPhWnKYxxX+X+p5r2xgzs4MPPthjnQMjR44M9tO0OP07j9PrNYXxz3/+czC21157eawp5nG6k7bLfOSRR4KxuKwjNXoe41Ke0aNHZ/w5nX967YlLhfQ6pKlqcfstTQGN2yfqudPWQKQb5o6eD003NAvTTjW1eMqUKcF+EydOzNPRIRd0vsyZM8djLXsxC1tIxW1DP/74Y4815R/rR9/nwYMHe/zmm28G+5X33lM/P+fPnx+MXXbZZR5ruvnuu+8e7NetWzeP//3vfwdjc+fO9TjF8kYtTRk6dGgw9t5773ms5b9mZgcccIDH5513nsd6T2q25j1/RehzjcbaFtzMbLPNNvM4ns8ov7h0UEvXNI7b6WnpxzfffBOM9ezZ02NNyy/EtqN8gw8AAAAAQAJ4wAcAAAAAIAFVuoq+pi7Fqaa6oq+msmjKqJnZ/vvv7/G3337rsaZMmYUp+5oyahamK2k6sa4sbma2zz77eByn1Gjasaa/aiqxWZjuEa9yqmlZKaboZ0sp1bIGPfczZ84M9iuElSmLlaYdxen1hx56qMdxCpuu9tqyZUuPTz/99GA/TRvV86bp+mZmDRs29Lhdu3bBWDy/V4tX/37yySc9fvfdd4OxFNMPyyvbv13H9Nobp81nWoU2Xk1W56n+jZiF10otq8nWVQTrRkspNCXfLDyH+v7HJRykfxan+B5m8eLFHsfp+1qyFK/cjtzQa2tFP3/0mhyXmT3zzDMe6z1vfB/VuXNnj+P7y/79+3sc//2kJj4Hej2M7yl19f1p06Z5fPnllwf7tWnTxmNNoY9L11Rchqv3uVoaF5dO0dGk4vQ9j0tYdO7ouYjvS/Qa+tBDDwVjL730kseFmJav+AYfAAAAAIAE8IAPAAAAAEACeMAHAAAAACABVdomL1M9illYj6T1NFqPb2bWpEkTj0888USP41ZfWuMyderUYEzrc7R1wt577x3spy28GjVqFIxpCw2ty4hrh/XfHLfdiOt1UlO3bl2PTzrppGAsriFdLa7frki7Q/xG6y+1paRZ2EbmqKOOCsa0Jl9r5OO/1+23397jVq1alfl7zcK5Xrt27WBM546e+759+wb79erVq8z9sP70eqvnLq7B1zrRuK5RWwzpa1Tn9RFyQT9DtTXrHnvsEeynNaK6xkLcdpSa7OKRqcWWWTiv4nUVdA0G5l9xiOfll19+6bF+Vrdu3TrYT9e30XZ6Zmbjxo3zWNfiqO73VHr/r2sTxe13L730Uo91zRltCWsWXqOzrQWg6yDErRVZa6ri9Dnj3HPPDcb0+U7F9zbPPvusx7rek1lx3W+m/UQJAAAAAEA1wQM+AAAAAAAJqNQU/ThdZdasWR6PGTMmGKtTp06ZcZzmr6ndW221lcfats4sTHlZunRpxrGaNWuW+XpmYSqOtigyy5zKGv8uLQ/44osvgrG4pV4KNB27Y8eOHmu7CrPwvdWWiV9//XWwHymlubFkyZJg+8UXX/RYU3/Nwjmmc1HTgM3KX2Ki5zA+nz/++KPHmorYs2fPYL/U2/wUovj6ranfcbmR/p1oiUe2lkJYO72eagug9u3bB/vp+6/XU20la0Z6bqHT863n+Mgjjwz209Ts+L7l/fff91jvszj3xUM/74YMGeLxCSecEOyn6claUmpmdvXVV3t84YUXehzfC1Rn+pkWtxLW913nm77nZms+GyhN7/7oo488fvnll4P9KKVZN/rcdskll3gcXye1tEnfY21fbmb22GOPeVxMKfkxvsEHAAAAACABPOADAAAAAJAAHvABAAAAAEhAldbga331jTfeGIwdfPDBHh966KEe77TTTsF+W265pcdaexbXB2uNd1wHqvVN2qYrrlfUenqtazQzmzhxoscjRozwOK6znzZtmsdxbYe2IEuF1hBqXajWOpmZ/fTTTx5rS8M33ngj2I8a/NyI38fJkyd7HNe7z5492+Pzzz/f42bNmgX76bmO214qvQ7E8+itt97y+Prrr/d43rx5GV8PVUPPY3xt1/peXVeB+bt+dF7Vr1/f47htmq5Vo3NMP6fMqMMudDVq1PBYW8u2bds22E9rUON7n/3228/jgQMHehx/BqNw6XXzgw8+8PiVV14J9tOa/LhVtLbS1M/uzz77LGfHmZJ4/Sxt1aufd3E7Qm0XHLe7++STTzy+8847PZ4/f/76HWw1E6/3pH/3ur5E3IZZ55GuefbEE08E+8VrfxUrvsEHAAAAACABPOADAAAAAJCAKu1ZpOkSc+bMCcb69+/vsbbwitsx6ba299I0GbMwVSNOC160aJHHixcv9lhTS83ClLZ4TFvj6X5x6qpuxy3/sqU1Fyt9L958802P4/dPW7F9/PHHHscppciPbHPx4Ycf9lhTzLp06RLs17hxY4/j1Cg1adIkj+MSjHfeecfj77//fm2HjSqkZTU6Z83C1nh6TuO0R6wbvZ4OHTrU43i+NW/e3OOxY8d6PGrUqGA/2jEVNj0/ej2MU381ZTVO99XyK8538dO2dr169QrGtBRVS1vNzL777juPaVe6dvFc0WeD3r17exyXOOi1N259PXr0aI91njIv142WZpuFaflbbLGFx/Gznt7b6jPmf/7zn2A/LdUuZnyDDwAAAABAAnjABwAAAAAgASXZUkNKSkrIG6k6o0pLS/fKxQsVynnUkoR4FUylKzsXe+pSaWlpydr3WrtCOYcqTvPTc5qt3ETTS4tkVfXk5mKuxR1ANGVcU4u1Y0llS20u6nzbZJNNMo5pumECqYfVai7qZ2bLli097tatW7Cfnn8taTQz+/zzzz3WEpmq/GxNbS5WlfhztlWrVh6fc845GX/uvvvu83jGjBkV/fXVai6Wl87ZYrh/Lca5qCW9ZuEq+LvvvrvHWg5qZnb33Xd7PHz4cI/jkuEiVOZc5Bt8AAAAAAASwAM+AAAAAAAJ4AEfAAAAAIAE0CsDlUbrkYqk9hpZaMsuVG8rVqwItrWFXjHUIRYjXaskfv+RBp07X331lcd33HFHxv107pmFfydIS3wf9fXXX3v8+OOPB2O69s28efPye2DVGJ93lW/cuHEez5w50+MBAwYE+40YMcLj+DqZIr7BBwAAAAAgATzgAwAAAACQAFL0ASBSbK1uCk2hvGerz2OhHA9QUZqOvWzZsio8kophLuaftsGM29/lol0mn4tpKPa5GLeHnDBhgsezZ8/2ePTo0cF+CbSJXSd8gw8AAAAAQAJ4wAcAAAAAIAE84AMAAAAAkABq8AvIJpts4rG2NEFx2XTTTc2serThSFWNGjU8pgVZ8dAaUbPf5yLnsHjpXKzO19Rir39efR6r8znMtfh6p7XJG2ywQcZ9y/u3FL9+rVq1PF66dOm6HSyqTAqfi/pviP+258+f7/HcuXM9jttIxu9DMSvP5yLf4AMAAAAAkAAe8AEAAAAASEDJWtJzFpjZtMo7HIgmpaWl9XPxQpzHKsM5TAPnsfhxDtPAeSx+nMM0cB6LH+cwDWWex6wP+AAAAAAAoDiQog8AAAAAQAJ4wAcAAAAAIAE84AMAAAAAkAAe8AEAAAAASAAP+AAAAAAAJOD/A6IG3UPJWcj+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x288 with 18 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test[:10])\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1,n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore model used for unsupervised pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 4)         292       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 4)           148       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 14, 14, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 8)         296       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 1)         73        \n",
      "=================================================================\n",
      "Total params: 889\n",
      "Trainable params: 889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_autoencoder = tf.keras.models.load_model(MODEL_PATH + 'autoencoder.h5')\n",
    "pretrained_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding layers at the end of the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing decoder (last 4 layers)\n",
    "x = pretrained_autoencoder.layers[-5].output\n",
    "# Adding layers\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(64, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "x = Conv2D(128, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "# Adding a fully connected layer for the 10 classes 0 to 9\n",
    "predictions = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 4)         292       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 4)           148       \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 7, 7, 32)          1184      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 2, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 161,010\n",
      "Trainable params: 161,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=pretrained_autoencoder.input, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### freezing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers of the pre-trained model\n",
    "# we will only update the weights for the added layers\n",
    "for layer in pretrained_autoencoder.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with adam optimizer and sparse_categorical_crossentropy loss function\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_for_training = 500\n",
    "X_train, y_train = x_train[:samples_for_training], y_train[:samples_for_training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 241us/sample - loss: 0.0076 - sparse_categorical_accuracy: 0.9990\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 244us/sample - loss: 0.0066 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 238us/sample - loss: 0.0055 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 214us/sample - loss: 0.0061 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 216us/sample - loss: 0.0054 - sparse_categorical_accuracy: 0.9990\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 211us/sample - loss: 0.0071 - sparse_categorical_accuracy: 0.9990\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 231us/sample - loss: 0.0053 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 214us/sample - loss: 0.0043 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 211us/sample - loss: 0.0037 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 213us/sample - loss: 0.0029 - sparse_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=100, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 192us/sample - loss: 0.4192 - sparse_categorical_accuracy: 0.9019\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test,y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net without pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing decoder (last 4 layers)\n",
    "input_images = Input(shape=(28, 28, 1)) \n",
    "# Adding layers\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_images)\n",
    "x = Conv2D(64, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "x = Conv2D(128, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "# Adding a fully connected layer for the 10 classes 0 to 9\n",
    "model_without_pretrained = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 896,906\n",
      "Trainable params: 896,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_basic = Model(inputs=input_images, outputs=model_without_pretrained)\n",
    "model_basic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with adam optimizer and sparse_categorical_crossentropy loss function\n",
    "model_basic.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 992us/sample - loss: 0.0113 - sparse_categorical_accuracy: 0.9980\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.0048 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.0021 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.0012 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 7.2854e-04 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 5.8663e-04 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 4.5476e-04 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 3.7774e-04 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 3.2419e-04 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 2.8823e-04 - sparse_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model_basic.fit(X_train, y_train, batch_size=100, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_basic.evaluate(x_test,y_test, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
