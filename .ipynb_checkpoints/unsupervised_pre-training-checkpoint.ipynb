{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MODEL_PATH = \"models/autoencoder/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0831 23:18:37.014140 11604 deprecation.py:506] From C:\\Users\\antho\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0831 23:18:37.151165 11604 deprecation.py:323] From C:\\Users\\antho\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(28, 28, 1))                                    # 28 x 28 x 1\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(input_img)    # 28 x 28 x 8\n",
    "x_M1 = MaxPooling2D((2, 2), padding='same')(x)                          # 14 x 14 x 8\n",
    "x_C2 = Conv2D(4, (3, 3), activation='relu', padding='same')(x_M1)       # 14 x 14 x 4\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x_C2)                    # 7 x 7 x 4 = (28 x 28 x 1) * 0.25 (and each feature map has the same replicated weights so it decreases again the complexity)\n",
    "\n",
    "x_C3 = Conv2D(4, (3, 3), activation='relu', padding='same')(encoded)    # 7 x 7 x 4\n",
    "x_U1 = UpSampling2D((2, 2))(x_C3)                                       # 14 x 14 x 4\n",
    "x_C4 = Conv2D(8, (3, 3), activation='relu', padding='same')(x_U1)      # 14 x 14 x 8\n",
    "x_U2 = UpSampling2D((2, 2))(x_C4)                                       # 28 x 28 x 8\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x_U2) # 28 x 28 x 1\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(8)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(8)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_M1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(4)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_C2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(7), Dimension(7), Dimension(4)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(7), Dimension(7), Dimension(4)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_C3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(4)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_U1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(8)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_C4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(8)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_U2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(1)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = tf.keras.models.load_model(MODEL_PATH + 'autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 52s 867us/sample - loss: 0.1200 - val_loss: 0.1189\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 58s 969us/sample - loss: 0.1200 - val_loss: 0.1188\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 68s 1ms/sample - loss: 0.1199 - val_loss: 0.1188\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1199 - val_loss: 0.1187\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1198 - val_loss: 0.1187\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 72s 1ms/sample - loss: 0.1198 - val_loss: 0.1187\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1197 - val_loss: 0.1186\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 72s 1ms/sample - loss: 0.1197 - val_loss: 0.1186\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 72s 1ms/sample - loss: 0.1197 - val_loss: 0.1185\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1196 - val_loss: 0.1185\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 69s 1ms/sample - loss: 0.1196 - val_loss: 0.1184\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1195 - val_loss: 0.1184\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1195 - val_loss: 0.1183\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1194 - val_loss: 0.1183\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1194 - val_loss: 0.1183\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1193 - val_loss: 0.1182\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1193 - val_loss: 0.1182\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 72s 1ms/sample - loss: 0.1193 - val_loss: 0.1181\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1192 - val_loss: 0.1181\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1192 - val_loss: 0.1180\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1191 - val_loss: 0.1180\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1191 - val_loss: 0.1180\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1190 - val_loss: 0.1179\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1190 - val_loss: 0.1179\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 70s 1ms/sample - loss: 0.1190 - val_loss: 0.1178\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 69s 1ms/sample - loss: 0.1189 - val_loss: 0.1178\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 68s 1ms/sample - loss: 0.1189 - val_loss: 0.1177\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1188 - val_loss: 0.1177\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1188 - val_loss: 0.1177\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1187 - val_loss: 0.1176\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 68s 1ms/sample - loss: 0.1187 - val_loss: 0.1176\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1187 - val_loss: 0.1175\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1186 - val_loss: 0.1175\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1186 - val_loss: 0.1175\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 68s 1ms/sample - loss: 0.1185 - val_loss: 0.1174\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1185 - val_loss: 0.1174\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1185 - val_loss: 0.1173\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1184 - val_loss: 0.1173\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1184 - val_loss: 0.1173\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1183 - val_loss: 0.1172\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 68s 1ms/sample - loss: 0.1183 - val_loss: 0.1172\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1183 - val_loss: 0.1171\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1182 - val_loss: 0.1171\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1182 - val_loss: 0.1171\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 68s 1ms/sample - loss: 0.1181 - val_loss: 0.1170\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1181 - val_loss: 0.1170\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1181 - val_loss: 0.1169\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1180 - val_loss: 0.1169\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1180 - val_loss: 0.1169\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 66s 1ms/sample - loss: 0.1179 - val_loss: 0.1168\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 68s 1ms/sample - loss: 0.1179 - val_loss: 0.1168\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1179 - val_loss: 0.1167\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1178 - val_loss: 0.1167\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 67s 1ms/sample - loss: 0.1178 - val_loss: 0.1167\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 66s 1ms/sample - loss: 0.1177 - val_loss: 0.1166\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 62s 1ms/sample - loss: 0.1177 - val_loss: 0.1166\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1177 - val_loss: 0.1165\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 66s 1ms/sample - loss: 0.1176 - val_loss: 0.1165\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 60s 1ms/sample - loss: 0.1176 - val_loss: 0.1165\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 64s 1ms/sample - loss: 0.1176 - val_loss: 0.1164\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 59s 985us/sample - loss: 0.1175 - val_loss: 0.1164\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 62s 1ms/sample - loss: 0.1175 - val_loss: 0.1164\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 63s 1ms/sample - loss: 0.1174 - val_loss: 0.1163\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 86s 1ms/sample - loss: 0.1164 - val_loss: 0.1153\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 85s 1ms/sample - loss: 0.1164 - val_loss: 0.1152\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 86s 1ms/sample - loss: 0.1163 - val_loss: 0.1152\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 87s 1ms/sample - loss: 0.1163 - val_loss: 0.1152\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 87s 1ms/sample - loss: 0.1163 - val_loss: 0.1151\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 83s 1ms/sample - loss: 0.1162 - val_loss: 0.1151\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 94s 2ms/sample - loss: 0.1162 - val_loss: 0.1151\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 90s 2ms/sample - loss: 0.1161 - val_loss: 0.1150\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 93s 2ms/sample - loss: 0.1161 - val_loss: 0.1150\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 88s 1ms/sample - loss: 0.1161 - val_loss: 0.1150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d00dcd54e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=100,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "autoencoder.save(MODEL_PATH + 'autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 18 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test[:10])\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1,n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore model used for unsupervised pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 4)         292       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 4)           148       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 14, 14, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 8)         296       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 1)         73        \n",
      "=================================================================\n",
      "Total params: 889\n",
      "Trainable params: 889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_autoencoder = tf.keras.models.load_model(MODEL_PATH + 'autoencoder.h5')\n",
    "pretrained_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding layers at the end of the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing decoder (last 4 layers)\n",
    "x = pretrained_autoencoder.layers[-5].output\n",
    "# Adding layers\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(64, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "x = Conv2D(128, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "# Adding a fully connected layer for the 10 classes 0 to 9\n",
    "predictions = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 4)         292       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 4)           148       \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 7, 7, 32)          1184      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 2, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 161,010\n",
      "Trainable params: 161,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=pretrained_autoencoder.input, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### freezing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers of the pre-trained model\n",
    "# we will only update the weights for the added layers\n",
    "for layer in pretrained_autoencoder.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with adam optimizer and sparse_categorical_crossentropy loss function\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_for_training = 500\n",
    "X_train, y_train = x_train[:samples_for_training], y_train[:samples_for_training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 241us/sample - loss: 0.0076 - sparse_categorical_accuracy: 0.9990\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 244us/sample - loss: 0.0066 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 238us/sample - loss: 0.0055 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 214us/sample - loss: 0.0061 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 216us/sample - loss: 0.0054 - sparse_categorical_accuracy: 0.9990\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 211us/sample - loss: 0.0071 - sparse_categorical_accuracy: 0.9990\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 231us/sample - loss: 0.0053 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 214us/sample - loss: 0.0043 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 211us/sample - loss: 0.0037 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 213us/sample - loss: 0.0029 - sparse_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=100, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 192us/sample - loss: 0.4192 - sparse_categorical_accuracy: 0.9019\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test,y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net without pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing decoder (last 4 layers)\n",
    "input_images = Input(shape=(28, 28, 1)) \n",
    "# Adding layers\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_images)\n",
    "x = Conv2D(64, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "x = Conv2D(128, (3, 3), strides=2, activation='relu', padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "# Adding a fully connected layer for the 10 classes 0 to 9\n",
    "model_without_pretrained = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 896,906\n",
      "Trainable params: 896,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_basic = Model(inputs=input_images, outputs=model_without_pretrained)\n",
    "model_basic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with adam optimizer and sparse_categorical_crossentropy loss function\n",
    "model_basic.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 992us/sample - loss: 0.0113 - sparse_categorical_accuracy: 0.9980\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.0048 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.0021 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.0012 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 7.2854e-04 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 5.8663e-04 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 4.5476e-04 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 3.7774e-04 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 3.2419e-04 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 2.8823e-04 - sparse_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model_basic.fit(X_train, y_train, batch_size=100, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_basic.evaluate(x_test,y_test, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
